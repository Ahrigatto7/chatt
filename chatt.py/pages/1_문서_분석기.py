import streamlit as st
from modules import file_handler, keyword_analyzer, rule_extractor, storage, database
import pandas as pd
import os
import json

CSV_PATH = 'Topic DB 216b50e66dba805abdb2ebd9cbd601da.csv'

def load_db_for_editing(filepath):
    expected_columns = ['Name', 'Description', 'Related Keywords', 'Related Examples']
    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
        return pd.read_csv(filepath)
    else:
        return pd.DataFrame(columns=expected_columns)

st.set_page_config(page_title="ë¬¸ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“– ë¬¸ì„œ ë¶„ì„ê¸°")
st.markdown("`.docx`, `.md`, `.txt` í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´, í•µì‹¬ í‚¤ì›Œë“œì™€ í•´ì„ ê·œì¹™ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("ğŸ“‚ ë¶„ì„í•  ë¬¸ì„œ ì—…ë¡œë“œ", type=["docx", "md", "txt"])

if uploaded_file:
    st.success(f"âœ… **{uploaded_file.name}** ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ!")

    text = file_handler.extract_text(uploaded_file)
    rules = rule_extractor.extract_interpretive_rules(text)
    extracted, keywords = keyword_analyzer.extract_sections(text)
    summary_df, keyword_map = keyword_analyzer.analyze_keywords(extracted, keywords)

    tab1, tab2 = st.tabs(["ğŸ” í‚¤ì›Œë“œ ë¶„ì„", "ğŸ“‘ í•´ì„ ê·œì¹™"])
    with tab1:
        st.subheader("í˜¼ì¸ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„")
        if not summary_df.empty:
            st.dataframe(summary_df, use_container_width=True)
        else:
            st.warning("ê´€ë ¨ëœ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    with tab2:
        st.subheader("ì‚¬ì£¼ í•´ì„ ê·œì¹™ ì¶”ì¶œ")
        if rules:
            st.json(rules)
        else:
            st.warning("í•´ì„ ê·œì¹™ì´ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")

    with st.expander("ğŸ§  ë¶„ì„ ê²°ê³¼ë¥¼ ì§€ì‹ DBì— ì¶”ê°€í•˜ê¸°"):
        st.markdown("ë°©ê¸ˆ ë¶„ì„í•œ ë‚´ìš©ì„ ìƒˆë¡œìš´ í† í”½ìœ¼ë¡œ ë§Œë“¤ì–´ ì§€ì‹ DBì— ì˜êµ¬ì ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        with st.form("upload_to_db_form"):
            default_description = json.dumps(rules, ensure_ascii=False, indent=2)
            default_examples = "\n\n".join([f"### {kw}\n" + "\n---\n".join(texts) for kw, texts in keyword_map.items()])
            default_keywords = ", ".join(keyword_map.keys())
            st.info("ì•„ë˜ ë‚´ìš©ì€ ë¶„ì„ ê²°ê³¼ë¡œ ìë™ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ììœ ë¡­ê²Œ ìˆ˜ì •í•˜ì—¬ ì €ì¥í•˜ì„¸ìš”.")
            topic_name = st.text_input("ìƒˆ í† í”½ ì´ë¦„ (Name)", placeholder="ì˜ˆ: í˜¼ì¸ê³¼ ê´€ë ¨ëœ ìƒˆë¡œìš´ ê·œì¹™")
            description = st.text_area("ì„¤ëª… (Description)", value=default_description, height=200)
            related_keywords = st.text_input("ê´€ë ¨ í‚¤ì›Œë“œ (Related Keywords)", value=default_keywords)
            related_examples = st.text_area("ê´€ë ¨ ì‚¬ë¡€ (Related Examples)", value=default_examples, height=200)

            if st.form_submit_button("ì§€ì‹ DBì— ì—…ë¡œë“œ"):
                if topic_name:
                    df = load_db_for_editing(CSV_PATH)
                    if not df['Name'].str.contains(topic_name).any():
                        new_row = pd.DataFrame([{'Name': topic_name, 'Description': description, 'Related Keywords': related_keywords, 'Related Examples': related_examples}])
                        df = pd.concat([df, new_row], ignore_index=True)
                        df.to_csv(CSV_PATH, index=False, encoding='utf-8-sig')
                        st.success(f"'{topic_name}' í† í”½ì´ ì§€ì‹ DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    else:
                        st.error(f"'{topic_name}'ì€(ëŠ”) ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í† í”½ ì´ë¦„ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
                else:
                    st.warning("ìƒˆ í† í”½ ì´ë¦„ì€ ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")